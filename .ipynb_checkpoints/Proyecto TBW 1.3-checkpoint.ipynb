{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roastme_dataset_path = \"Corpus_builder_and_pre/Dataset/\"\n",
    "balance_dataset_path = \"Corpus_builder_and_pre/Neutros&Positivos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Tips para construir corpus para entrenamiento</h6>\n",
    "<ul>\n",
    "<li>Razon documentos Agresivos / No Agresivos: 6500/4500</li>\n",
    "<li>Stopwords: Agresivos incluyen \"key_words\" {\"you\", \"your\", \"he\", \"she\", \"it\"}, No Agresivos solo incluyen stopwords normal</li>\n",
    "<li>Se aumenta el peso de \"badwords\" y \"ngrams\" x10</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badwords List Size: 376\n",
      "Creating the corpus...\n",
      "Adding Roastme Dataset...\n",
      "Adding Neutral&Positive Dataset...\n",
      "Roastme Dataset Size: 6501 docs\n",
      "Balance Dataset Size: 4500 docs\n",
      "Trainning Corpus Size: 8800\n",
      "Test Corpus Size: 2201\n"
     ]
    }
   ],
   "source": [
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "print(\"Badwords List Size: \"+str(len(bad_words)))\n",
    "        \n",
    "#Remove keywords in cyberbullying from stopwords list\n",
    "keywords = [\"you\", \"your\", \"he\", \"she\", \"it\"]\n",
    "stopword_list = [stopword for stopword in stopwords.words('english') if stopword not in keywords]\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create the corpus\n",
    "print(\"Creating the corpus...\")\n",
    "mcorpus = dict()\n",
    "mcorpus['roastme'] = list()\n",
    "mcorpus['balance'] = list()\n",
    "\n",
    "print(\"Adding Roastme Dataset...\")\n",
    "count = 0\n",
    "for name in os.listdir(roastme_dataset_path):\n",
    "    if (count >= 6500):\n",
    "        break\n",
    "    if name.endswith('.json'):\n",
    "        with open(roastme_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extrae el comentario\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Elimina los links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "\n",
    "                    tokens_comment = [word for word in comment_text.lower().split() if word not in stopword_list]\n",
    "                    mcorpus['roastme'].append(tokens_comment)\n",
    "                    \n",
    "                    count += 1\n",
    "                    if (count >= 6500):\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                            tokens_comment = [word for word in comment_text.lower().split() if word not in stopword_list]\n",
    "                            mcorpus['roastme'].append(tokens_comment)\n",
    "                            \n",
    "                            count += 1\n",
    "                            if (count >= 6500):\n",
    "                                break\n",
    "                            \n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "print(\"Adding Neutral&Positive Dataset...\")\n",
    "count = 0\n",
    "for name in os.listdir(balance_dataset_path):\n",
    "    if (count >= 4500):\n",
    "        break\n",
    "    if name.endswith('.json'):\n",
    "        with open(balance_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extrae el comentario\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Elimina los links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "\n",
    "                    tokens_comment = [word for word in comment_text.lower().split() if word not in stopwords.words('english')]\n",
    "                    mcorpus['balance'].append(tokens_comment)\n",
    "                    \n",
    "                    count += 1\n",
    "                    if (count >= 4500):\n",
    "                        break\n",
    "                \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                            tokens_comment = [word for word in comment_text.lower().split() if word not in stopwords.words('english')]\n",
    "                            mcorpus['balance'].append(tokens_comment)\n",
    "                            \n",
    "                            count += 1\n",
    "                            if (count >= 4500):\n",
    "                                break\n",
    "                            \n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "#Experimentation Cross-Validation: 80-20 (80% for training and 20% for testing)\n",
    "print(\"Roastme Dataset Size: \"+str(len(mcorpus['roastme']))+\" docs\") #Agressive\n",
    "print(\"Balance Dataset Size: \"+str(len(mcorpus['balance']))+\" docs\") #Non Agressive\n",
    "\n",
    "roastme_trainning_size = int(len(mcorpus['roastme'])*0.8)\n",
    "balance_trainning_size = int(len(mcorpus['balance'])*0.8)\n",
    "\n",
    "train_corpus = mcorpus['roastme'][:roastme_trainning_size] + mcorpus['balance'][:balance_trainning_size]\n",
    "test_corpus = mcorpus['roastme'][roastme_trainning_size:] + mcorpus['balance'][balance_trainning_size:]\n",
    "test_corpus_only_bad = mcorpus['roastme'][roastme_trainning_size:]\n",
    "test_corpus_only_balance = mcorpus['balance'][balance_trainning_size:]\n",
    "\n",
    "print(\"Trainning Corpus Size: \"+str(len(train_corpus)))\n",
    "print(\"Test Corpus Size: \"+str(len(test_corpus)))\n",
    "\n",
    "#Remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for doc in train_corpus:\n",
    "    for token in doc:\n",
    "        frequency[token] += 1\n",
    "docs = [[token for token in doc if frequency[token] > 1] for doc in train_corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)\n",
    "\n",
    "#Create and save dictionary\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "#dictionary.filter_extremes(no_below=20, no_above=0.8) # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "#Increasing weight of bad words:\n",
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "ngrams_ids = [word_id for word_id, word in dictionary.iteritems() if \"_\" in word]\n",
    "\n",
    "for doc_idx in range(len(corpus)):\n",
    "    word_id_list = []\n",
    "    freq_list = []\n",
    "\n",
    "    for word_id, freq in corpus[doc_idx]:\n",
    "        word_id_list.append(word_id)\n",
    "        if (word_id in bad_words_ids or word_id in ngrams_ids):\n",
    "            freq_list.append(freq*10)\n",
    "        else:\n",
    "            freq_list.append(freq)\n",
    "    \n",
    "    new_doc = zip(word_id_list,freq_list)\n",
    "    corpus[doc_idx] = new_doc\n",
    "\n",
    "print(\"Saving Corpus and Dictionary...\")\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict') #Save the dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus) #Save the corpus\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary, alpha='auto')\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determinando topico con mayo relevancia de badwords (solo para analizar)\n",
    "topic1 = []\n",
    "topic2 = []\n",
    "for bad_word_id in bad_words_ids:\n",
    "    #print(dictionary[bad_word_id])\n",
    "    try:\n",
    "        topics = lda_model.get_term_topics(bad_word_id,0) \n",
    "        topic1.append(topics[0][1])\n",
    "        topic2.append(topics[1][1])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(sum(topic1))\n",
    "print(sum(topic2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate with roastme dataset (agressive)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in test_corpus_only_bad]\n",
    "total_docs = 0\n",
    "true_positive = 0\n",
    "false_negative = 0\n",
    "percent_list = []\n",
    "for docbow in corpus:\n",
    "    total_docs += 1\n",
    "    \n",
    "    if (len(lda_model[docbow]) == 2):\n",
    "        if (lda_model[docbow][1][1] > 0.5):\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "    else:\n",
    "        if (lda_model[docbow][0][0] == 1):\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "recall = true_positive/(true_positive+false_negative)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "#print('{0:.16f}'.format(recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for doc_idx in range(len(mcorpus['roastme'])):\n",
    "#    if doc_idx > 100:\n",
    "#        break\n",
    "#    print(mcorpus['roastme'][doc_idx])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "#lda = models.LdaModel.load('tmp/cyberbullying_ldaModel.lda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
