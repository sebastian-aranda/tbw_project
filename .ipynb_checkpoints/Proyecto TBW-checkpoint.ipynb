{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = \"cyberbullying_corpus/\"\n",
    "#corpus_path = \"corpus_lda/corpus_lda.lda_c\"\n",
    "#dictionary_path =\"corpus_lda/corpus_lda.dict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python2.7/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "#Remove keywords in cyberbullying from stopwords list\n",
    "#keywords = [\"you\", \"your\", \"he\", \"she\", \"it\"]\n",
    "stopword_list = [stopword for stopword in stopwords.words('english') if stopword not in keywords]\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create the corpus\n",
    "corpus = []\n",
    "for filename in os.listdir(corpus_path):\n",
    "    with open(corpus_path+filename) as file:\n",
    "        tokens = []\n",
    "        for line in file:\n",
    "            for sym in puncts:\n",
    "                line = line.replace(sym,\" \")\n",
    "            for num in digits:\n",
    "                line = line.replace(num,\" \")\n",
    "\n",
    "            tokens = tokens + [word for word in line.lower().split() if word not in stopword_list]\n",
    "        corpus.append(tokens)\n",
    "\n",
    "#Remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in corpus:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in corpus]\n",
    "\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts = [[lemmatizer.lemmatize(token) for token in doc] for doc in texts]\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(texts, min_count=20)\n",
    "for idx in range(len(texts)):\n",
    "    print(bigram[texts[idx]])\n",
    "    for token in bigram[texts[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts[idx].append(token)\n",
    "\n",
    "#Create and save dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict') #Save the dictionary\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus) #Save the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'slice' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-c02139207b26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sebastian/anaconda3/lib/python2.7/site-packages/gensim/models/phrases.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"For a faster implementation, use the gensim.models.phrases.Phraser class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mis_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_single\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# if the input is an entire corpus (rather than a single sentence),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sebastian/anaconda3/lib/python2.7/site-packages/gensim/models/phrases.pyc\u001b[0m in \u001b[0;36m_is_single\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0miterable\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mobj_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'slice' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(bigram[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Increasing weight of bad words:\n",
    "bad_words = [\"ass\", \"face\", \"dick\", \"asshole\"]\n",
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "i = 0\n",
    "for doc in corpus[:10]:\n",
    "    i = i+1\n",
    "    word_id_list = []\n",
    "    freq_list = []\n",
    "    \n",
    "    j = 0\n",
    "    for word_id, freq in doc:\n",
    "        word_id_list.append(word_id)\n",
    "        if word_id in bad_words_ids:\n",
    "            freq_list.append(freq*2000)\n",
    "        else:\n",
    "            freq_list.append(freq)\n",
    "        \n",
    "        j = j+1\n",
    "    \n",
    "    print(\"Most frequent word of Doc_\"+str(i))\n",
    "    #print max(freq_list)\n",
    "    print dictionary[word_id_list[freq_list.index(max(freq_list))]]\n",
    "    #print bad_words_ids.index(word_id_list[freq_list.index(max(freq_list))])\n",
    "    #print bad_words[bad_words_ids.index(word_id_list[freq_list.index(max(freq_list))])]\n",
    "    print(\"----------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topFrequentWords(defaultdictionary):\n",
    "    i = 0\n",
    "    for word, freq in defaultdictionary.items():\n",
    "        if freq > 1000:\n",
    "            print word\n",
    "\n",
    "topFrequentWords(frequency)\n",
    "#print(texts[:10])\n",
    "#print(dictionary[0])\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#%time tfidf = models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "#Create a LDA Model and save it\n",
    "lda = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda.save('tmp/cyberbullying_ldaModel.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "lda = models.LdaModel.load('tmp/cyberbullying_ldaModel.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Topics\n",
    "lda.print_topics(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
