{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.classify import PositiveNaiveBayesClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "#import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Rutas a Datasets</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roastme_dataset_path = \"Corpus_builder_and_pre/Dataset/Bullying\"\n",
    "positive_dataset_path = \"Corpus_builder_and_pre/Dataset/NoBullying\"\n",
    "#balance_dataset_path = \"Corpus_builder_and_pre/Neutros&Positivos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Construcción del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset completo:\n",
      "97718\n"
     ]
    }
   ],
   "source": [
    "#Strange symbols\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords_v2.txt','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.split('-')[0].strip()\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "#Create the corpus\n",
    "corpus = list()\n",
    "for name in os.listdir(positive_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(positive_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]        \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "positive_dataset_size = len(corpus)\n",
    "\n",
    "for name in os.listdir(roastme_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(roastme_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    \n",
    "                    #Constraint for dataset size equality\n",
    "                    if (len(corpus) >= 2*positive_dataset_size):\n",
    "                        break\n",
    "                    \n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            \n",
    "                            if (len(corpus) >= 2*positive_dataset_size):\n",
    "                                break\n",
    "                            \n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        #print(e)\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "\n",
    "print(\"Tamaño del dataset completo:\")\n",
    "print(len(corpus))\n",
    "#TODO: guardar corpus de texto con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48859\n"
     ]
    }
   ],
   "source": [
    "print(positive_dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Procesamiento del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove lone letters\n",
    "corpus = [[word for word in doc if len(word)>1] for doc in corpus]\n",
    "\n",
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "# Save comments after process\n",
    "comments_pro = list()\n",
    "for comment in corpus:\n",
    "    comments_pro.append(' '.join(comment))\n",
    "    \n",
    "#Get Trigrams\n",
    "#bigram = Phrases(corpus)\n",
    "#trigram = Phrases(bigram[corpus])\n",
    "#for idx in range(len(corpus)):\n",
    "#    for token in trigram[corpus[idx]]:\n",
    "#            if '_' in token:\n",
    "#                corpus[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Transformación del Corpus a espacio de vectores</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "#dictionary.filter_extremes(no_below=len(dictionary)*0.001, no_above=0.75)\n",
    "dictionary.filter_extremes()\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "#Save corpus and dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Características para Clasificación</h1>\n",
    "<ul>\n",
    "<li>Porcentaje de pertenencia al tópico generado de LDA en Roastme Dataset</li>\n",
    "<li>Densidad de Badwords</li>\n",
    "<li>Polaridad del post utilizando Sentiment Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Wall time: 0 ns\n",
      "Creating corpus...\n",
      "Wall time: 93 ms\n"
     ]
    }
   ],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA Features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Modelo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 468 ms, total: 2min 19s\n",
      "Wall time: 2min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.055*\"look\" + 0.055*\"like\" + 0.014*\"face\" + 0.009*\"hair\" + 0.007*\"roast\" + 0.006*\"post\" + 0.006*\"would\" + 0.006*\"eye\" + 0.005*\"fucking\" + 0.005*\"please\" + 0.004*\"boy\" + 0.004*\"gay\" + 0.004*\"say\" + 0.004*\"guy\" + 0.004*\"know\" + 0.004*\"shit\" + 0.004*\"think\" + 0.004*\"use\" + 0.004*\"fat\" + 0.004*\"love\" + 0.003*\"want\" + 0.003*\"eyebrow\" + 0.003*\"oh\" + 0.003*\"head\" + 0.003*\"subreddit\" + 0.003*\"chin\" + 0.003*\"brother\" + 0.003*\"sex\" + 0.003*\"fuck\" + 0.003*\"comment\" + 0.003*\"report\" + 0.003*\"teeth\" + 0.003*\"sure\" + 0.003*\"ugly\" + 0.003*\"god\" + 0.003*\"one\" + 0.002*\"child\" + 0.002*\"deleted\" + 0.002*\"mouth\" + 0.002*\"thought\"'),\n",
       " (1,\n",
       "  u'0.019*\"like\" + 0.014*\"look\" + 0.012*\"one\" + 0.011*\"get\" + 0.009*\"guy\" + 0.008*\"girl\" + 0.007*\"kid\" + 0.007*\"make\" + 0.006*\"got\" + 0.006*\"time\" + 0.006*\"school\" + 0.006*\"would\" + 0.006*\"even\" + 0.005*\"see\" + 0.005*\"year\" + 0.005*\"know\" + 0.005*\"back\" + 0.005*\"going\" + 0.005*\"picture\" + 0.005*\"go\" + 0.005*\"thing\" + 0.005*\"life\" + 0.005*\"friend\" + 0.005*\"tell\" + 0.005*\"could\" + 0.005*\"take\" + 0.005*\"fuck\" + 0.004*\"bet\" + 0.004*\"good\" + 0.004*\"right\" + 0.004*\"dick\" + 0.004*\"still\" + 0.004*\"left\" + 0.004*\"mom\" + 0.004*\"people\" + 0.004*\"ever\" + 0.004*\"old\" + 0.004*\"need\" + 0.004*\"parent\" + 0.004*\"think\"')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "#LDA-2\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(2,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-3\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(3,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-5\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(5,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-10\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(10,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creación de características</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_mostAgressiveTopic = 0; \n",
    "\n",
    "lda_features = []\n",
    "for docbow in corpus:\n",
    "    lda_features.append(lda_model[docbow][id_mostAgressiveTopic][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bad Words Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "bad_words_features = []\n",
    "\n",
    "for docbow in corpus:\n",
    "        bad_words_features.append(sum([freq for word_id,freq in docbow if word_id in bad_words_ids])/len(docbow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment Analysis Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dataset_size = int(len(corpus)/2)\n",
    "positive_comments = [\" \".join(comments_pro[i]) for i in range(positive_dataset_size)]\n",
    "negative_comments = [\" \".join(comments_pro[i]) for i in range(positive_dataset_size,len(corpus))]\n",
    "\n",
    "def features(list_of_comments):\n",
    "    words = list_of_comments.lower().split()\n",
    "    return dict(('contains(%s)' % w, True) for w in words)\n",
    "\n",
    "positive_featuresets = list(map(features, negative_comments)) + list(map(features, bad_words))\n",
    "#positive_featuresets = list(map(features, bad_words))\n",
    "unlabeled_featuresets = list(map(features, positive_comments))\n",
    "classifier = PositiveNaiveBayesClassifier.train(positive_featuresets, unlabeled_featuresets)\n",
    "\n",
    "sentiment_features = list()\n",
    "for doc in comments_pro:\n",
    "    sentiment_features.append(classifier.classify(\" \".join(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(str(classifier.classify(features('You are nice'))) + '\\n' +\n",
    "      str(classifier.classify(features('you are a scumbag mf'))) + '\\n' +\n",
    "      str(classifier.classify(features('You are beautiful'))) + '\\n' +\n",
    "      str(classifier.classify(features('How much did your lip injections cost'))) + '\\n' +\n",
    "      str(classifier.classify(features('You re what every girl wants a Gay Asian best friend'))) + '\\n' +\n",
    "      str(classifier.classify(features('sloths are the most dumb people'))) + '\\n' +\n",
    "      str(classifier.classify(features('sloths are the smartest people'))) + '\\n' +\n",
    "      str(classifier.classify(features('I think you did a great job'))) + '\\n' +\n",
    "      str(classifier.classify(features('You look like a twink KennyS'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Etiquetas</h3>\n",
    "<p>Asignar etiqueta segun corpus</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136523\n"
     ]
    }
   ],
   "source": [
    "positive_dataset_size = len(corpus)/2\n",
    "y1 = [0 for label in range(positive_dataset_size)]\n",
    "y2 = [1 for label in range(positive_dataset_size,len(corpus))]\n",
    "y = y1 + y2\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606862201388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = zip(lda_features,bad_words_features, sentiment_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .7)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "myClassifier = KNeighborsClassifier()\n",
    "myClassifier.fit(X_train, y_train)\n",
    "predictions = myClassifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
