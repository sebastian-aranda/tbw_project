{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Rutas a Datasets</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roastme_dataset_path = \"Corpus_builder_and_pre/Dataset/\"\n",
    "positive_dataset_path = \"Corpus_builder_and_pre/Positivos/\"\n",
    "balance_dataset_path = \"Corpus_builder_and_pre/Neutros&Positivos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Construcción del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anus', 'arse', 'arsehole', 'ass', 'ass', 'ass', 'ass', 'assbag', 'assbandit', 'assbanger', 'assbite', 'assclown', 'asscock', 'asscracker', 'asses', 'assface', 'assfuck', 'assfucker', 'assgoblin', 'asshat', 'asshead', 'asshole', 'asshopper', 'assjacker', 'asslick', 'asslicker', 'assmonkey', 'assmunch', 'assmuncher', 'assnigger', 'asspirate', 'assshit', 'assshole', 'asssucker', 'asswad', 'asswipe', 'axwound', 'bampot', 'bastard', 'beaner', 'bitch', 'bitchass', 'bitches', 'bitchtits', 'bitchy', 'blow job', 'blowjob', 'bollocks', 'bollox', 'boner', 'brotherfucker', 'bullshit', 'bumblefuck', 'butt plug', 'butt', 'buttfucka', 'buttfucker', 'camel toe', 'carpetmuncher', 'chesticle', 'chinc', 'chink', 'choad', 'chode', 'clit', 'clitface', 'clitfuck', 'clusterfuck', 'cock', 'cockass', 'cockbite', 'cockburger', 'cockface', 'cockfucker', 'cockhead', 'cockjockey', 'cockknoker', 'cockmaster', 'cockmongler', 'cockmongruel', 'cockmonkey', 'cockmuncher', 'cocknose', 'cocknugget', 'cockshit', 'cocksmith', 'cocksmoke', 'cocksmoker', 'cocksniffer', 'cocksucker', 'cockwaffle', 'coochie', 'coochy', 'coon', 'cooter', 'cracker', 'cum', 'cumbubble', 'cumdumpster', 'cumguzzler', 'cumjockey', 'cumslut', 'cumtart', 'cunnie', 'cunnilingus', 'cunt', 'cuntass', 'cuntface', 'cunthole', 'cuntlicker', 'cuntrag', 'cuntslut', 'dago', 'damn', 'deggo', 'dick', 'dick', 'dickbag', 'dickbeaters', 'dickface', 'dickfuck', 'dickfucker', 'dickhead', 'dickhole', 'dickjuice', 'dickmilk', 'dickmonger', 'dicks', 'dickslap', 'dicksucker', 'dicksucking', 'dicktickler', 'dickwad', 'dickweasel', 'dickweed', 'dickwod', 'dike', 'dildo', 'dipshit', 'doochbag', 'dookie', 'douche', 'douche', 'douchebag', 'douchewaffle', 'dumass', 'dumb ass', 'dumbass', 'dumbfuck', 'dumbshit', 'dumshit', 'dyke', 'fag', 'fagbag', 'fagfucker', 'faggit', 'faggot', 'faggotcock', 'fagtard', 'fatass', 'fellatio', 'feltch', 'flamer', 'fuck', 'fuckass', 'fuckbag', 'fuckboy', 'fuckbrain', 'fuckbutt', 'fuckbutter', 'fucked', 'fucker', 'fuckersucker', 'fuckface', 'fuckhead', 'fuckhole', 'fuckin', 'fucking', 'fucknut', 'fucknutt', 'fuckoff', 'fucks', 'fuckstick', 'fucktard', 'fucktart', 'fuckup', 'fuckwad', 'fuckwit', 'fuckwitt', 'fudgepacker', 'gay', 'gayass', 'gaybob', 'gaydo', 'gayfuck', 'gayfuckist', 'gaylord', 'gaytard', 'gaywad', 'goddamn', 'goddamnit', 'gooch', 'gook', 'gringo', 'guido', 'handjob', 'hard on', 'heeb', 'hell', 'ho', 'hoe', 'homo', 'homodumbshit', 'honkey', 'humping', 'jackass', 'jagoff', 'jap', 'jerk off', 'jerkass', 'jigaboo', 'jizz', 'jungle bunny', 'junglebunny', 'kike', 'kooch', 'kootch', 'kraut', 'kunt', 'kyke', 'lameass', 'lardass', 'lesbian', 'lesbo', 'lezzie', 'mcfagget', 'mick', 'minge', 'mothafucka', 'mothafuckin', 'motherfucker', 'motherfucking', 'muff', 'muffdiver', 'munging', 'negro', 'nigaboo', 'nigga', 'nigger', 'niggers', 'niglet', 'nut sack', 'nutsack', 'paki', 'panooch', 'pecker', 'peckerhead', 'penis', 'penisbanger', 'penisfucker', 'penispuffer', 'piss', 'pissed', 'pissed off', 'pissflaps', 'polesmoker', 'pollock', 'poon', 'poonani', 'poonany', 'poontang', 'porch monkey', 'porchmonkey', 'prick', 'punanny', 'punta', 'pussies', 'pussy', 'pussylicking', 'puto', 'queef', 'queer', 'queerbait', 'queerhole', 'sand nigger', 'sandnigger', 'schlong', 'scrote', 'shit', 'shitass', 'shitbag', 'shitbagger', 'shitbrains', 'shitbreath', 'shitcanned', 'shitcunt', 'shitdick', 'shitface', 'shitfaced', 'shithead', 'shithole', 'shithouse', 'shitspitter', 'shitstain', 'shitter', 'shittiest', 'shitting', 'shitty', 'shiz', 'shiznit', 'skank', 'skeet', 'skullfuck', 'slut', 'slutbag', 'smeg', 'snatch', 'spic', 'spick', 'splooge', 'spook', 'suckass', 'tard', 'testicle', 'thundercunt', 'tit', 'titfuck', 'tits', 'tittyfuck', 'twat', 'twatlips', 'twats', 'twatwaffle', 'vag', 'vagina', 'wank', 'wankjob', 'wetback', 'whore', 'whorebag', 'whoreface', 'wop']\n",
      "Dataset Size\n",
      "605\n"
     ]
    }
   ],
   "source": [
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords_v2.txt','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.split('-')[0].strip()\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "#Create the corpus\n",
    "corpus = list()\n",
    "for name in os.listdir(roastme_dataset_path):\n",
    "    if (len(corpus)>300):\n",
    "        break\n",
    "    if name.endswith('.json'):\n",
    "        with open(roastme_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                    \n",
    "                    if (randint(1,10)<=8):\n",
    "                        tokens_comment.append(bad_words[randint(0,len(bad_words)-1)])\n",
    "                        tokens_comment.append(bad_words[randint(0,len(bad_words)-1)])\n",
    "                        \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            \n",
    "                            if (randint(1,10)<=8):\n",
    "                                tokens_comment.append(bad_words[randint(0,len(bad_words)-1)])\n",
    "                                tokens_comment.append(bad_words[randint(0,len(bad_words)-1)])\n",
    "                                \n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        #print(e)\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "\n",
    "for name in os.listdir(positive_dataset_path):\n",
    "    if (len(corpus)>600):\n",
    "        break\n",
    "    if name.endswith('.json'):\n",
    "        with open(positive_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]        \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "print(\"Dataset Size\")\n",
    "print(len(corpus))\n",
    "#TODO: guardar corpus de texto con pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Procesamiento del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove lone letters\n",
    "corpus = [[word for word in doc if len(word)>1] for doc in corpus]\n",
    "\n",
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "#Get Trigrams\n",
    "bigram = Phrases(corpus)\n",
    "trigram = Phrases(bigram[corpus])\n",
    "for idx in range(len(corpus)):\n",
    "    for token in trigram[corpus[idx]]:\n",
    "            if '_' in token:\n",
    "                corpus[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Transformación del Corpus a espacio de vectores</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=len(dictionary)*0.001, no_above=0.75)\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "#Save corpus and dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Características para Clasificación</h1>\n",
    "<ul>\n",
    "<li>Porcentaje de pertenencia al tópico generado de LDA en Roastme Dataset</li>\n",
    "<li>Densidad de Badwords</li>\n",
    "<li>TODO: Polaridad</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.46 s, sys: 4 ms, total: 4.46 s\n",
      "Wall time: 4.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.026*\"like\" + 0.021*\"look\" + 0.016*\"2 girls 1 cup\" + 0.016*\"anal\" + 0.015*\"2g1c\" + 0.014*\"acrotomophilia\" + 0.013*\"alaskan pipeline\" + 0.011*\"apeshit\" + 0.011*\"anus\" + 0.010*\"look_like\" + 0.009*\"anilingus\" + 0.009*\"alabama hot pocket\" + 0.008*\"hair\" + 0.008*\"face\" + 0.008*\"know\" + 0.007*\"really\" + 0.007*\"guy\" + 0.006*\"even\" + 0.006*\"would\" + 0.006*\"good\" + 0.006*\"thing\" + 0.006*\"ugly\" + 0.006*\"want\" + 0.006*\"arsehole\" + 0.005*\"get\" + 0.005*\"self\" + 0.005*\"looking\" + 0.005*\"got\" + 0.005*\"way\" + 0.005*\"people\" + 0.005*\"go\" + 0.005*\"long\" + 0.005*\"bad\" + 0.005*\"girl\" + 0.005*\"still\" + 0.005*\"well\" + 0.005*\"amish\" + 0.005*\"damn\" + 0.005*\"nice\" + 0.005*\"also\" + 0.004*\"one\" + 0.004*\"first\" + 0.004*\"make\" + 0.004*\"picture\" + 0.004*\"friend\" + 0.004*\"shit\" + 0.004*\"say\" + 0.004*\"mean\" + 0.004*\"think\" + 0.004*\"much\" + 0.004*\"time\" + 0.003*\"wow\" + 0.003*\"hot\" + 0.003*\"year\" + 0.003*\"beard\" + 0.003*\"holy\" + 0.003*\"halloween\" + 0.003*\"could\" + 0.003*\"better\" + 0.003*\"post\" + 0.003*\"though\" + 0.003*\"roast\" + 0.003*\"ever\" + 0.003*\"man\" + 0.003*\"dude\" + 0.003*\"change\" + 0.003*\"glass\" + 0.003*\"making\" + 0.003*\"take\" + 0.003*\"back\" + 0.003*\"actually\" + 0.003*\"hairstyle\" + 0.003*\"lot\" + 0.003*\"life\" + 0.003*\"party\" + 0.002*\"started\" + 0.002*\"gay\" + 0.002*\"day\" + 0.002*\"tell\" + 0.002*\"gonna\" + 0.002*\"lady\" + 0.002*\"job\" + 0.002*\"right\" + 0.002*\"haircut\" + 0.002*\"head\" + 0.002*\"dog\" + 0.002*\"need\" + 0.002*\"another\" + 0.002*\"big\" + 0.002*\"always\" + 0.002*\"fucking\" + 0.002*\"see\" + 0.002*\"enough\" + 0.002*\"pretty\" + 0.002*\"looked\" + 0.002*\"suit\" + 0.002*\"last\" + 0.002*\"improvement\" + 0.002*\"beautiful\" + 0.002*\"night\"'),\n",
       " (1,\n",
       "  u'0.029*\"like\" + 0.027*\"look\" + 0.022*\"look_like\" + 0.021*\"arsehole\" + 0.016*\"apeshit\" + 0.016*\"anilingus\" + 0.015*\"alabama hot pocket\" + 0.014*\"alaskan pipeline\" + 0.011*\"acrotomophilia\" + 0.011*\"get\" + 0.010*\"2g1c\" + 0.009*\"one\" + 0.008*\"2 girls 1 cup\" + 0.008*\"year\" + 0.007*\"anus\" + 0.006*\"ugly\" + 0.006*\"anal\" + 0.005*\"think\" + 0.005*\"dude\" + 0.005*\"would\" + 0.005*\"picture\" + 0.005*\"nice\" + 0.005*\"transformation\" + 0.005*\"life\" + 0.005*\"could\" + 0.004*\"photo\" + 0.004*\"amish\" + 0.004*\"kid\" + 0.004*\"better\" + 0.004*\"time\" + 0.004*\"change\" + 0.004*\"doritos\" + 0.004*\"still\" + 0.004*\"girl\" + 0.004*\"great\" + 0.004*\"also\" + 0.004*\"see\" + 0.004*\"fuck\" + 0.004*\"hair\" + 0.004*\"much\" + 0.004*\"never\" + 0.004*\"say\" + 0.004*\"good\" + 0.003*\"school\" + 0.003*\"halloween\" + 0.003*\"work\" + 0.003*\"take\" + 0.003*\"right\" + 0.003*\"man\" + 0.003*\"ever\" + 0.003*\"tell\" + 0.003*\"feel\" + 0.003*\"looking\" + 0.003*\"even\" + 0.003*\"yeah\" + 0.003*\"mountain\" + 0.003*\"holy\" + 0.003*\"always\" + 0.003*\"go\" + 0.003*\"way\" + 0.003*\"got\" + 0.003*\"kind\" + 0.003*\"first\" + 0.003*\"though\" + 0.003*\"leprechaun\" + 0.003*\"post\" + 0.003*\"brother\" + 0.003*\"protect\" + 0.003*\"dew\" + 0.002*\"last\" + 0.002*\"looked\" + 0.002*\"pretty\" + 0.002*\"made\" + 0.002*\"make\" + 0.002*\"people\" + 0.002*\"thought\" + 0.002*\"comment\" + 0.002*\"open\" + 0.002*\"bro\" + 0.002*\"going\" + 0.002*\"day\" + 0.002*\"top\" + 0.002*\"shirt\" + 0.002*\"guy\" + 0.002*\"mom\" + 0.002*\"wow\" + 0.002*\"lot\" + 0.002*\"improvement\" + 0.002*\"depression\" + 0.002*\"source\" + 0.002*\"reddit\" + 0.002*\"face\" + 0.002*\"actually\" + 0.002*\"really\" + 0.002*\"together\" + 0.002*\"fucking\" + 0.002*\"beard\" + 0.002*\"costume\" + 0.002*\"since\" + 0.002*\"new\"')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(2,100)\n",
    "#doc1 = \"hola maricon holapera\".split()\n",
    "#doc1bow = dictionary.doc2bow(doc1)\n",
    "#print(lda_model[doc1bow])\n",
    "\n",
    "#lda_features = []\n",
    "#for docbow in corpus:\n",
    "#    lda_features.append(lda_model[docbow][0])\n",
    "#print(lda_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print(lda_model[dictionary.doc2bow(\"bitch kid\".split())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bad Words Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-98a22da4e349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbad_words_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdocbow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mbad_words_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfreq\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocbow\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_words_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbad_words_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "\n",
    "bad_words_features = []\n",
    "for docbow in corpus:\n",
    "        bad_words_features.append(sum([freq for word_id,freq in docbow if word_id in bad_words_ids]))\n",
    "print(bad_words_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = zip(lda_features,bad_words_features)\n",
    "y = [1 for doc in corpus] #Supuesto todos son agresivos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .7)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighborsClassifier.fit(X_train, y_train)\n",
    "predictions = KNeighborsClassifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
