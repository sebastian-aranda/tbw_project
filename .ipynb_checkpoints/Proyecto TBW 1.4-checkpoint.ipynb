{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Rutas a Datasets</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roastme_dataset_path = \"Corpus_builder_and_pre/Dataset/\"\n",
    "positive_dataset_path = \"Corpus_builder_and_pre/Positivos/\"\n",
    "#balance_dataset_path = \"Corpus_builder_and_pre/Neutros&Positivos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Construcción del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roastme Dataset Len: 87664\n",
      "Dataset Size\n",
      "87996\n"
     ]
    }
   ],
   "source": [
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords_v2.txt','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.split('-')[0].strip()\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "#Create the corpus\n",
    "corpus = list()\n",
    "for name in os.listdir(roastme_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(roastme_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        #print(e)\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "\n",
    "roastme_len = len(corpus)\n",
    "print(\"Roastme Dataset Len: \"+str(roastme_len))\n",
    "\n",
    "for name in os.listdir(positive_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(positive_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]        \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "positive_len = len(corpus)-roastme_len\n",
    "print(\"Dataset Size\")\n",
    "print(len(corpus))\n",
    "#TODO: guardar corpus de texto con pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Procesamiento del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove lone letters\n",
    "corpus = [[word for word in doc if len(word)>1] for doc in corpus]\n",
    "\n",
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "#Get Trigrams\n",
    "#bigram = Phrases(corpus)\n",
    "#trigram = Phrases(bigram[corpus])\n",
    "#for idx in range(len(corpus)):\n",
    "#    for token in trigram[corpus[idx]]:\n",
    "#            if '_' in token:\n",
    "#                corpus[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Transformación del Corpus a espacio de vectores</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MultiProcessing\n",
    "#import multiprocessing as mp\n",
    "\n",
    "#Create dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=len(dictionary)*0.001, no_above=0.75)\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "#Save corpus and dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Características para Clasificación</h1>\n",
    "<ul>\n",
    "<li>Porcentaje de pertenencia al tópico generado de LDA en Roastme Dataset</li>\n",
    "<li>Densidad de Badwords</li>\n",
    "<li>TODO: Polaridad</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.04 ms\n",
      "Creating corpus...\n",
      "CPU times: user 32 ms, sys: 16 ms, total: 48 ms\n",
      "Wall time: 49 ms\n"
     ]
    }
   ],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "#%time lda_model = ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "#lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "#lda_model.print_topics(10,10)\n",
    "\n",
    "#lda_features = []\n",
    "#for docbow in corpus:\n",
    "#    lda_features.append(lda_model[docbow])\n",
    "lda_features = [randint(0,5) for random_number in range(len(corpus))]\n",
    "\n",
    "#print(lda_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bad Words Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "bad_words_features = []\n",
    "\n",
    "for docbow in corpus:\n",
    "        bad_words_features.append(sum([freq for word_id,freq in docbow if word_id in bad_words_ids]))\n",
    "#print(bad_words_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Etiquetas</h3>\n",
    "<p>Asignar etiqueta segun corpus</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87996\n"
     ]
    }
   ],
   "source": [
    "y1 = [1 for label in range(roastme_len)]\n",
    "y2 = [0 for label in range(roastme_len,len(corpus))]\n",
    "y = y1 + y2\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99618494107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = zip(lda_features,bad_words_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .7)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "myClassifier = KNeighborsClassifier()\n",
    "myClassifier.fit(X_train, y_train)\n",
    "predictions = myClassifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
