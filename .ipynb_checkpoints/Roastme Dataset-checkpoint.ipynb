{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Dataset\n"
     ]
    }
   ],
   "source": [
    "path = 'Corpus_builder_and_pre/Dataset/'\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create the corpus\n",
    "print(\"Reading Dataset\")\n",
    "corpus = list()\n",
    "for name in os.listdir(path):\n",
    "    if (len(corpus)>5000):\n",
    "        break\n",
    "    if name.endswith('.json'):\n",
    "        with open(path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {u'sex_change': 6, u'r_roastme': 7, u'black_guy': 10, u'let_guess': 7, u'friend_zone': 7, u'pm_cube': 1, u'community_college': 7, u'extra_chromosome': 9, u'post_history': 6, u'mountain_dew': 8, u'middle_school': 6, u'justin_bieber': 8, u'pubic_hair': 7, u'thank_god': 6, u'oh_god': 8, u'moderator_message': 6, u'jesus_christ': 7, u'un_roastable': 6, u'doge_apprentice': 1, u'andy_milonakis': 6, u'removal_please': 6, u'good_thing': 9, u'birth_control': 6, u'holy_shit': 9, u'pretty_sure': 21, u'properly_question': 6, u'type_guy': 14, u'please_send': 1, u'action_working': 6, u'suck_dick': 8, u'macaulay_culkin': 6, u'facial_hair': 8, u'violating_subreddits': 6, u'get_laid': 8, u'someone_else': 9, u'even_though': 7, u'retarded_brother': 6, u'bot_remove': 6, u'cant_tell': 6, u'bad_news': 19, u'next_time': 8, u'piece_paper': 9, u'make_cry': 10, u'self_esteem': 7, u'daddy_issue': 6, u'oh_wait': 7, u'bot_message': 6, u'kind_guy': 15, u'reply_inform': 6, u'go_back': 12, u'rule_issue': 6, u'moderator_taken': 6, u'roastme_unlimited': 1, u'ever_seen': 12, u'school_shooting': 6, u'make_fun': 9, u'year_old': 58, u'_': 5, u'post_respond': 6, u'big_enough': 6, u'one_day': 16, u'kind_person': 8, u'high_school': 36, u'post_removed': 6, u'love_child': 6, u'harry_potter': 13, u'look_like': 912, u'holding_sign': 6, u'never_seen': 10, u'overly_attached': 6, u'receding_hairline': 7, u'send_aforementioned': 6, u'called_want': 8, u'please_use': 6, u'like_type': 1, u'donald_trump': 9})\n",
      "defaultdict(<type 'int'>, {u'_': 1553})\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "#Get Phrases\n",
    "bigram = Phrases(corpus)\n",
    "trigram = Phrases(bigram[corpus])\n",
    "#print(bigram[corpus][:10])\n",
    "trigrams = defaultdict(int)\n",
    "bigrams = defaultdict(int)\n",
    "for doc in corpus:\n",
    "    for token in bigram[doc]:\n",
    "        if '_' in token:\n",
    "            bigrams[token] += 1        \n",
    "    for token in trigram[bigram[doc]]:\n",
    "        if '_' in token:\n",
    "            trigrams[token] += 1\n",
    "\n",
    "print(bigrams)\n",
    "print(trigrams)\n",
    "\n",
    "exit\n",
    "#Create and save dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=len(dictionary)*0.001, no_above=0.75) # Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "\n",
    "freq_dictionary = defaultdict(int)\n",
    "for doc in corpus:\n",
    "    for token in doc:\n",
    "        if token in dictionary.values():\n",
    "            freq_dictionary[token] += 1\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(freq_dictionary.items()[:10])\n",
    "sorted_dict = sorted(freq_dictionary.items(), key=operator.itemgetter(1))\n",
    "desc_sorted_dict = list(reversed(sorted_dict))\n",
    "print(desc_sorted_dict[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.031*\"like\" + 0.030*\"look\" + 0.016*\"post\" + 0.013*\"think\" + 0.013*\"need\" + 0.012*\"please\" + 0.011*\"report\" + 0.011*\"one\" + 0.011*\"subreddit\" + 0.011*\"would\" + 0.010*\"make\" + 0.009*\"already\" + 0.008*\"fucked\" + 0.008*\"hard\" + 0.008*\"asian\" + 0.008*\"teeth\" + 0.008*\"user\" + 0.008*\"side\" + 0.008*\"hate\" + 0.008*\"rule\" + 0.008*\"bot\" + 0.008*\"money\" + 0.008*\"message\" + 0.008*\"removed\" + 0.008*\"moderator\" + 0.008*\"allowed\" + 0.008*\"violation\" + 0.006*\"roast\" + 0.006*\"say\" + 0.006*\"see\" + 0.006*\"eye\" + 0.006*\"year\" + 0.005*\"go\" + 0.005*\"kid\" + 0.005*\"friend\" + 0.005*\"someone\" + 0.005*\"never\" + 0.005*\"face\" + 0.005*\"people\" + 0.005*\"man\" + 0.005*\"dick\" + 0.005*\"first\" + 0.005*\"get\" + 0.005*\"u\" + 0.005*\"use\" + 0.005*\"kind\" + 0.004*\"least\" + 0.004*\"woman\" + 0.004*\"deleted\" + 0.004*\"always\" + 0.004*\"mouth\" + 0.004*\"another\" + 0.004*\"person\" + 0.004*\"finger\" + 0.004*\"lot\" + 0.004*\"behind\" + 0.004*\"working\" + 0.004*\"world\" + 0.004*\"comment\" + 0.004*\"might\" + 0.004*\"family\" + 0.004*\"bit\" + 0.004*\"taken\" + 0.004*\"puberty\" + 0.004*\"action\" + 0.004*\"fun\" + 0.004*\"matter\" + 0.004*\"literally\" + 0.004*\"issue\" + 0.004*\"seems\" + 0.004*\"easy\" + 0.004*\"buy\" + 0.004*\"act\" + 0.004*\"able\" + 0.004*\"owner\" + 0.004*\"link\" + 0.004*\"properly\" + 0.004*\"question\" + 0.004*\"bro\" + 0.004*\"history\" + 0.004*\"manage\" + 0.004*\"send\" + 0.004*\"bigger\" + 0.004*\"roastme\" + 0.004*\"laid\" + 0.004*\"personal\" + 0.004*\"car\" + 0.004*\"remove\" + 0.004*\"information\" + 0.004*\"comedy\" + 0.004*\"roaster\" + 0.004*\"inform\" + 0.004*\"mask\" + 0.004*\"respond\" + 0.004*\"gene\" + 0.004*\"rely\" + 0.004*\"touch\" + 0.004*\"reply\" + 0.004*\"roastee\" + 0.004*\"aforementioned\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = ldamodel.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "lda_model.print_topics(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
