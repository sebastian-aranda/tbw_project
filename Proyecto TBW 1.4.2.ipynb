{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Rutas a Datasets</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_dataset_path = \"Corpus_builder_and_pre/Dataset/Bullying\"\n",
    "positive_dataset_path = \"Corpus_builder_and_pre/Dataset/NoBullying\"\n",
    "#balance_dataset_path = \"Corpus_builder_and_pre/Neutros&Positivos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Construcción del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48859\n",
      "97719\n"
     ]
    }
   ],
   "source": [
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords_v2.txt','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.split('-')[0].strip()\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "#Create the corpus\n",
    "corpus = list()\n",
    "for name in os.listdir(positive_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(positive_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]        \n",
    "                    corpus.append(tokens_comment)\n",
    "                    \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "positive_dataset_size = len(corpus)\n",
    "print(len(corpus))\n",
    "\n",
    "for name in os.listdir(negative_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(negative_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:\n",
    "                    \n",
    "                    if (len(corpus)>2*positive_dataset_size):\n",
    "                        break\n",
    "                    \n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                    corpus.append(tokens_comment)\n",
    "                    \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            if (len(corpus)>2*positive_dataset_size):\n",
    "                                break\n",
    "                            \n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                            corpus.append(tokens_comment)             \n",
    "                    except Exception as e:\n",
    "                        #print(e)\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Procesamiento del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove lone letters\n",
    "corpus = [[word for word in doc if len(word)>1] for doc in corpus]\n",
    "\n",
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "#Get Trigrams\n",
    "#bigram = Phrases(corpus)\n",
    "#trigram = Phrases(bigram[corpus])\n",
    "#for idx in range(len(corpus)):\n",
    "#    for token in trigram[corpus[idx]]:\n",
    "#            if '_' in token:\n",
    "#                corpus[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Transformación del Corpus a espacio de vectores</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes()\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "#Save corpus and dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Características para Clasificación</h1>\n",
    "<ul>\n",
    "<li>Porcentaje de pertenencia al tópico generado de LDA en Roastme Dataset</li>\n",
    "<li>Densidad de Badwords</li>\n",
    "<li>TODO: Polaridad</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 674 µs\n",
      "Creating corpus...\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.66 ms\n",
      "Corpus Size: 601\n"
     ]
    }
   ],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "    print(\"Corpus Size: \"+str(len(corpus)))\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 41s, sys: 364 ms, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.048*\"like\" + 0.042*\"look\" + 0.011*\"one\" + 0.010*\"get\" + 0.009*\"face\" + 0.009*\"guy\" + 0.007*\"would\" + 0.006*\"girl\" + 0.006*\"make\" + 0.006*\"know\" + 0.005*\"kid\" + 0.005*\"fuck\" + 0.005*\"think\" + 0.005*\"even\" + 0.005*\"say\" + 0.005*\"roast\" + 0.005*\"time\" + 0.005*\"got\" + 0.005*\"school\" + 0.004*\"tell\" + 0.004*\"go\" + 0.004*\"people\" + 0.004*\"going\" + 0.004*\"year\" + 0.004*\"head\" + 0.004*\"thing\" + 0.004*\"right\" + 0.004*\"good\" + 0.004*\"see\" + 0.004*\"life\" + 0.004*\"friend\" + 0.004*\"could\" + 0.004*\"back\" + 0.004*\"bet\" + 0.004*\"already\" + 0.004*\"still\" + 0.003*\"left\" + 0.003*\"shit\" + 0.003*\"want\" + 0.003*\"probably\"'),\n",
       " (1,\n",
       "  u'0.013*\"hair\" + 0.009*\"post\" + 0.008*\"look\" + 0.007*\"face\" + 0.007*\"please\" + 0.005*\"like\" + 0.005*\"oh\" + 0.005*\"eyebrow\" + 0.004*\"love\" + 0.004*\"use\" + 0.004*\"subreddit\" + 0.004*\"forehead\" + 0.004*\"comment\" + 0.004*\"eye\" + 0.004*\"teeth\" + 0.004*\"report\" + 0.004*\"black\" + 0.004*\"god\" + 0.004*\"deleted\" + 0.003*\"see\" + 0.003*\"thought\" + 0.003*\"guy\" + 0.003*\"hate\" + 0.003*\"rule\" + 0.003*\"user\" + 0.003*\"would\" + 0.003*\"moderator\" + 0.003*\"as\" + 0.003*\"new\" + 0.003*\"well\" + 0.003*\"got\" + 0.003*\"gay\" + 0.003*\"removed\" + 0.003*\"never\" + 0.003*\"message\" + 0.003*\"bot\" + 0.003*\"another\" + 0.003*\"top\" + 0.003*\"u\" + 0.003*\"photo\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "#LDA-2\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(2,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.44 s, sys: 12 ms, total: 3.45 s\n",
      "Wall time: 3.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.028*\"hair\" + 0.028*\"like\" + 0.024*\"really\" + 0.021*\"get\" + 0.020*\"looking\" + 0.018*\"change\" + 0.017*\"girl\" + 0.017*\"good\" + 0.016*\"look\" + 0.016*\"beard\" + 0.016*\"also\" + 0.016*\"right\" + 0.015*\"always\" + 0.014*\"think\" + 0.013*\"want\" + 0.013*\"wow\" + 0.013*\"better\" + 0.013*\"school\" + 0.013*\"tell\" + 0.012*\"got\"'),\n",
       " (1,\n",
       "  u'0.056*\"like\" + 0.030*\"look\" + 0.028*\"get\" + 0.020*\"still\" + 0.018*\"face\" + 0.018*\"picture\" + 0.017*\"thing\" + 0.014*\"ever\" + 0.013*\"year\" + 0.013*\"even\" + 0.013*\"much\" + 0.013*\"damn\" + 0.013*\"mean\" + 0.013*\"holy\" + 0.012*\"see\" + 0.012*\"would\" + 0.012*\"transformation\" + 0.012*\"make\" + 0.012*\"time\" + 0.011*\"say\"'),\n",
       " (2,\n",
       "  u'0.101*\"look\" + 0.084*\"like\" + 0.024*\"ugly\" + 0.018*\"nice\" + 0.018*\"one\" + 0.017*\"would\" + 0.015*\"could\" + 0.015*\"think\" + 0.015*\"year\" + 0.014*\"amish\" + 0.014*\"life\" + 0.013*\"go\" + 0.013*\"way\" + 0.012*\"fuck\" + 0.011*\"guy\" + 0.011*\"know\" + 0.011*\"great\" + 0.011*\"face\" + 0.010*\"first\" + 0.010*\"self\"')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA-3\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 384 ms, total: 3min 22s\n",
      "Wall time: 3min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.018*\"post\" + 0.015*\"please\" + 0.015*\"gay\" + 0.015*\"dad\" + 0.009*\"chin\" + 0.009*\"subreddit\" + 0.008*\"u\" + 0.008*\"report\" + 0.008*\"let\" + 0.007*\"rule\" + 0.007*\"user\" + 0.007*\"moderator\" + 0.006*\"message\" + 0.006*\"bot\" + 0.006*\"another\" + 0.006*\"violation\" + 0.006*\"girl\" + 0.006*\"daddy\" + 0.006*\"never\" + 0.005*\"old\"'),\n",
       " (1,\n",
       "  u'0.049*\"like\" + 0.040*\"look\" + 0.019*\"roast\" + 0.014*\"guy\" + 0.013*\"school\" + 0.012*\"bet\" + 0.011*\"dick\" + 0.010*\"as\" + 0.010*\"fuck\" + 0.009*\"friend\" + 0.009*\"kid\" + 0.008*\"girl\" + 0.007*\"would\" + 0.007*\"fat\" + 0.007*\"nice\" + 0.006*\"say\" + 0.006*\"want\" + 0.006*\"one\" + 0.005*\"got\" + 0.005*\"people\"'),\n",
       " (2,\n",
       "  u'0.019*\"make\" + 0.015*\"face\" + 0.009*\"eyebrow\" + 0.008*\"day\" + 0.008*\"one\" + 0.008*\"made\" + 0.007*\"see\" + 0.007*\"know\" + 0.007*\"hair\" + 0.006*\"mouth\" + 0.006*\"year\" + 0.006*\"think\" + 0.006*\"like\" + 0.006*\"bad\" + 0.006*\"time\" + 0.005*\"feel\" + 0.005*\"even\" + 0.005*\"get\" + 0.005*\"would\" + 0.005*\"thing\"'),\n",
       " (3,\n",
       "  u'0.084*\"look\" + 0.083*\"like\" + 0.016*\"face\" + 0.014*\"hair\" + 0.009*\"head\" + 0.008*\"eye\" + 0.008*\"would\" + 0.008*\"guy\" + 0.007*\"forehead\" + 0.007*\"shit\" + 0.007*\"ugly\" + 0.007*\"got\" + 0.006*\"dude\" + 0.006*\"shirt\" + 0.006*\"fucking\" + 0.005*\"nose\" + 0.005*\"get\" + 0.005*\"already\" + 0.005*\"picture\" + 0.005*\"child\"'),\n",
       " (4,\n",
       "  u'0.016*\"one\" + 0.015*\"get\" + 0.010*\"life\" + 0.008*\"could\" + 0.008*\"mom\" + 0.008*\"girl\" + 0.008*\"boy\" + 0.007*\"thing\" + 0.007*\"time\" + 0.007*\"going\" + 0.007*\"tell\" + 0.007*\"take\" + 0.007*\"face\" + 0.007*\"know\" + 0.006*\"even\" + 0.006*\"ever\" + 0.006*\"much\" + 0.006*\"would\" + 0.006*\"right\" + 0.006*\"parent\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA-5\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 8 ms, total: 2.56 s\n",
      "Wall time: 2.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.078*\"look\" + 0.062*\"like\" + 0.034*\"thing\" + 0.034*\"guy\" + 0.028*\"hair\" + 0.023*\"abraham\" + 0.023*\"lincoln\" + 0.023*\"make\" + 0.023*\"picture\" + 0.023*\"kid\" + 0.022*\"really\" + 0.017*\"year\" + 0.017*\"still\" + 0.017*\"work\" + 0.017*\"last\" + 0.017*\"people\" + 0.017*\"seen\" + 0.017*\"chin\" + 0.012*\"ugly\" + 0.012*\"nice\"'),\n",
       " (1,\n",
       "  u'0.061*\"like\" + 0.038*\"know\" + 0.031*\"also\" + 0.027*\"would\" + 0.026*\"post\" + 0.023*\"comment\" + 0.019*\"look\" + 0.019*\"face\" + 0.019*\"think\" + 0.019*\"got\" + 0.019*\"long\" + 0.019*\"looking\" + 0.019*\"still\" + 0.019*\"halloween\" + 0.019*\"thing\" + 0.019*\"girl\" + 0.016*\"guy\" + 0.016*\"go\" + 0.016*\"amp\" + 0.016*\"really\"'),\n",
       " (2,\n",
       "  u'0.049*\"like\" + 0.042*\"wow\" + 0.035*\"look\" + 0.028*\"improvement\" + 0.021*\"though\" + 0.021*\"doritos\" + 0.021*\"oh\" + 0.021*\"need\" + 0.015*\"get\" + 0.015*\"time\" + 0.015*\"much\" + 0.015*\"want\" + 0.015*\"always\" + 0.015*\"think\" + 0.015*\"nice\" + 0.015*\"could\" + 0.015*\"looking\" + 0.015*\"u\" + 0.015*\"photo\" + 0.015*\"take\"'),\n",
       " (3,\n",
       "  u'0.057*\"get\" + 0.048*\"look\" + 0.046*\"like\" + 0.032*\"amish\" + 0.027*\"glass\" + 0.022*\"fat\" + 0.022*\"picture\" + 0.022*\"bro\" + 0.016*\"face\" + 0.016*\"hair\" + 0.016*\"party\" + 0.016*\"take\" + 0.016*\"better\" + 0.016*\"best\" + 0.016*\"friend\" + 0.016*\"bad\" + 0.015*\"nice\" + 0.014*\"try\" + 0.014*\"thought\" + 0.014*\"well\"'),\n",
       " (4,\n",
       "  u'0.061*\"look\" + 0.037*\"like\" + 0.033*\"think\" + 0.029*\"even\" + 0.029*\"ugly\" + 0.025*\"really\" + 0.021*\"time\" + 0.021*\"self\" + 0.021*\"dude\" + 0.017*\"hair\" + 0.017*\"looking\" + 0.017*\"want\" + 0.017*\"say\" + 0.017*\"year\" + 0.017*\"boy\" + 0.017*\"go\" + 0.017*\"school\" + 0.017*\"shit\" + 0.014*\"good\" + 0.013*\"pretty\"'),\n",
       " (5,\n",
       "  u'0.141*\"like\" + 0.138*\"look\" + 0.043*\"one\" + 0.025*\"year\" + 0.022*\"big\" + 0.021*\"first\" + 0.018*\"got\" + 0.017*\"girl\" + 0.014*\"ugly\" + 0.014*\"great\" + 0.014*\"face\" + 0.014*\"leprechaun\" + 0.014*\"fuck\" + 0.014*\"looked\" + 0.014*\"change\" + 0.011*\"transformation\" + 0.011*\"hair\" + 0.011*\"better\" + 0.011*\"duckling\" + 0.011*\"see\"'),\n",
       " (6,\n",
       "  u'0.043*\"one\" + 0.034*\"like\" + 0.033*\"get\" + 0.030*\"ever\" + 0.027*\"never\" + 0.026*\"yeah\" + 0.025*\"see\" + 0.025*\"also\" + 0.025*\"brother\" + 0.021*\"would\" + 0.020*\"girl\" + 0.019*\"ugly\" + 0.019*\"time\" + 0.019*\"tell\" + 0.019*\"two\" + 0.019*\"beard\" + 0.016*\"know\" + 0.015*\"feel\" + 0.014*\"change\" + 0.013*\"roast\"'),\n",
       " (7,\n",
       "  u'0.042*\"like\" + 0.030*\"year\" + 0.030*\"even\" + 0.030*\"holy\" + 0.024*\"would\" + 0.024*\"old\" + 0.024*\"face\" + 0.024*\"amish\" + 0.018*\"look\" + 0.018*\"get\" + 0.018*\"long\" + 0.018*\"want\" + 0.018*\"fuck\" + 0.018*\"kind\" + 0.018*\"gonna\" + 0.013*\"guy\" + 0.013*\"go\" + 0.013*\"make\" + 0.013*\"day\" + 0.013*\"ugly\"'),\n",
       " (8,\n",
       "  u'0.035*\"nice\" + 0.029*\"like\" + 0.029*\"life\" + 0.029*\"could\" + 0.029*\"damn\" + 0.023*\"look\" + 0.023*\"picture\" + 0.023*\"good\" + 0.023*\"people\" + 0.018*\"even\" + 0.018*\"way\" + 0.018*\"work\" + 0.018*\"job\" + 0.018*\"sorry\" + 0.018*\"make\" + 0.018*\"hot\" + 0.018*\"dad\" + 0.018*\"doritos\" + 0.018*\"dude\" + 0.018*\"transformation\"'),\n",
       " (9,\n",
       "  u'0.046*\"look\" + 0.040*\"get\" + 0.035*\"say\" + 0.035*\"hair\" + 0.029*\"like\" + 0.029*\"parent\" + 0.023*\"dog\" + 0.023*\"great\" + 0.023*\"night\" + 0.023*\"beard\" + 0.018*\"one\" + 0.018*\"bad\" + 0.018*\"job\" + 0.018*\"good\" + 0.018*\"face\" + 0.018*\"dude\" + 0.018*\"thank\" + 0.012*\"ugly\" + 0.012*\"self\" + 0.012*\"well\"')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LDA-10\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_features = []\n",
    "for docbow in corpus:\n",
    "    lda_features.append(lda_model[docbow][0][1])\n",
    "\n",
    "#print(lda_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bad Words Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "bad_words_features = []\n",
    "\n",
    "for docbow in corpus:\n",
    "        bad_words_features.append(sum([freq for word_id,freq in docbow if word_id in bad_words_ids]))\n",
    "#print(bad_words_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Etiquetas</h3>\n",
    "<p>Asignar etiqueta segun corpus</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601\n"
     ]
    }
   ],
   "source": [
    "y1 = [0 for label in range(48859)]\n",
    "y2 = [1 for label in range(48859,len(corpus))]\n",
    "y = y1 + y2\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515439429929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = zip(lda_features,bad_words_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .7)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "myClassifier = KNeighborsClassifier()\n",
    "myClassifier.fit(X_train, y_train)\n",
    "predictions = myClassifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def classifyPost(document):\n",
    "#LDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
