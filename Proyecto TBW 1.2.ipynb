{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = \"cyberbullying_corpus/\"\n",
    "dataset_path = \"Corpus_builder_and_pre/Dataset/\"\n",
    "#corpus_path = \"corpus_lda/corpus_lda.lda_c\"\n",
    "#dictionary_path =\"corpus_lda/corpus_lda.dict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Badwords List Size: 376\n",
      "Creating the corpus...\n",
      "Exception at file:5 has no children\n",
      "Exception at file:115 has no children\n",
      "Exception at file:293 has no children\n",
      "Exception at file:390 has no children\n",
      "Exception at file:507 has no children\n",
      "Exception at file:694 has no children\n",
      "Exception at file:963 has no children\n",
      "Exception at file:1107 has no children\n",
      "Exception at file:1528 has no children\n",
      "Exception at file:1617 has no children\n",
      "Exception at file:1789 has no children\n",
      "Exception at file:1820 has no children\n",
      "Exception at file:2185 has no children\n",
      "Exception at file:2211 has no children\n",
      "Exception at file:2267 has no children\n",
      "Exception at file:2382 has no children\n",
      "Exception at file:2444 has no children\n",
      "Exception at file:2509 has no children\n",
      "Exception at file:2657 has no children\n",
      "Exception at file:2665 has no children\n",
      "Exception at file:2965 has no children\n",
      "Exception at file:2976 has no children\n",
      "Exception at file:2995 has no children\n",
      "Exception at file:3171 has no children\n",
      "Exception at file:3686 has no children\n",
      "Exception at file:3745 has no children\n",
      "Exception at file:3806 has no children\n",
      "Exception at file:3898 has no children\n",
      "Exception at file:4020 has no children\n",
      "Exception at file:4151 has no children\n",
      "Exception at file:4168 has no children\n",
      "Exception at file:4788 has no children\n",
      "Exception at file:4849 has no children\n",
      "Exception at file:4927 has no children\n",
      "Exception at file:4944 has no children\n",
      "Exception at file:5059 has no children\n",
      "Saving Corpus...\n"
     ]
    }
   ],
   "source": [
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "print(\"Badwords List Size: \"+str(len(bad_words)))\n",
    "        \n",
    "#Remove keywords in cyberbullying from stopwords list\n",
    "keywords = [\"you\", \"your\", \"he\", \"she\", \"it\"]\n",
    "stopword_list = [stopword for stopword in stopwords.words('english') if stopword not in keywords]\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create the corpus\n",
    "print(\"Creating the corpus...\")\n",
    "mcorpus = []\n",
    "count = 0\n",
    "for name in os.listdir(dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(dataset_path+'/'+name) as f:\n",
    "            count += 1\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:\n",
    "                    tokens = [] \n",
    "                    \n",
    "                    #Extrae el comentario\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Elimina los links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "\n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "\n",
    "                    tokens_comment = [word for word in comment_text.lower().split() if word not in stopword_list]\n",
    "                    tokens = tokens + tokens_comment\n",
    "\n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "\n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "\n",
    "                            tokens_comment = [word for word in comment_text.lower().split() if word not in stopword_list]\n",
    "                            tokens = tokens + tokens_comment\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    mcorpus.append(tokens)\n",
    "            except Exception:\n",
    "                print('Exception at file:'+str(count)+' has no children')\n",
    "                pass\n",
    "        \n",
    "#Remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for doc in mcorpus:\n",
    "    for token in doc:\n",
    "        frequency[token] += 1\n",
    "docs = [[token for token in doc if frequency[token] > 1] for doc in mcorpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            docs[idx].append(token)\n",
    "\n",
    "#Create and save dictionary\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict') #Save the dictionary\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "#Increasing weight of bad words:\n",
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "ngrams_ids = [word_id for word_id, word in dictionary.iteritems() if \"_\" in word]\n",
    "\n",
    "for doc_idx in range(len(corpus)):\n",
    "    word_id_list = []\n",
    "    freq_list = []\n",
    "\n",
    "    for word_id, freq in corpus[doc_idx]:\n",
    "        word_id_list.append(word_id)\n",
    "        #if (word_id in bad_words_ids and randint(0,9) > 5):\n",
    "        if (word_id in bad_words_ids):\n",
    "            freq_list.append(freq*3)\n",
    "        else:\n",
    "            freq_list.append(freq)\n",
    "    \n",
    "    new_doc = zip(word_id_list,freq_list)\n",
    "    corpus[doc_idx] = new_doc\n",
    "\n",
    "print(\"Saving Corpus...\")\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus) #Save the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1)], [(10, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)], [(10, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(34, 1), (35, 1), (36, 1), (37, 1), (38, 1)], [(3, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)], [(3, 1), (7, 1), (10, 6), (20, 1), (33, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 2), (52, 1), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1)], [(1, 1), (10, 2), (33, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 2), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1)], [(3, 1), (10, 3), (20, 1), (39, 1), (69, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1)], [(50, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1)], [(10, 1), (25, 1), (33, 1), (74, 1), (97, 300)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 12.7 ms\n",
      "Creating corpus...\n",
      "CPU times: user 120 ms, sys: 4 ms, total: 124 ms\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    %time dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    %time corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 50s, sys: 64 ms, total: 2min 50s\n",
      "Wall time: 2min 50s\n"
     ]
    }
   ],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#%time tfidf = models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "#Create a LDA Model and save it\n",
    "%time lda_2 = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda.save('tmp/cyberbullying_ldaModel.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 47s, sys: 52 ms, total: 2min 47s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "#Create a LDA Model and save it\n",
    "%time lda_3 = ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 184 ms, total: 2min 57s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "#Create a LDA Model and save it\n",
    "%time lda_4 = ldamodel.LdaModel(corpus, num_topics=4, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 148 ms, total: 2min 57s\n",
      "Wall time: 2min 57s\n"
     ]
    }
   ],
   "source": [
    "#Create a LDA Model and save it\n",
    "%time lda_5 = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "lda = models.LdaModel.load('tmp/cyberbullying_ldaModel.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.047*\"you\" + 0.031*\"your\" + 0.018*\"fuck\" + 0.016*\"shit\" + 0.016*\"it\" + 0.014*\"fucking\" + 0.008*\"sex\" + 0.007*\"she\" + 0.007*\"get\" + 0.007*\"he\" + 0.006*\"one\" + 0.006*\"time\" + 0.006*\"roast\" + 0.005*\"bitch\" + 0.005*\"know\" + 0.005*\"u\" + 0.005*\"face\" + 0.005*\"picture\" + 0.005*\"make\" + 0.005*\"hair\" + 0.004*\"see\" + 0.004*\"even\" + 0.004*\"post\" + 0.004*\"porn\" + 0.004*\"pussy\" + 0.004*\"think\" + 0.003*\"sure\" + 0.003*\"never\" + 0.003*\"good\" + 0.003*\"say\" + 0.003*\"suck\" + 0.003*\"take\" + 0.003*\"life\" + 0.003*\"use\" + 0.003*\"well\" + 0.003*\"could\" + 0.003*\"would\" + 0.003*\"thing\" + 0.003*\"cock\" + 0.003*\"oh\" + 0.003*\"first\" + 0.003*\"need\" + 0.003*\"looking\" + 0.003*\"please\" + 0.003*\"ever\" + 0.003*\"faggot\" + 0.002*\"guy\" + 0.002*\"rape\" + 0.002*\"going\" + 0.002*\"r\" + 0.002*\"thought\" + 0.002*\"seen\" + 0.002*\"deleted\" + 0.002*\"people\" + 0.002*\"probably\" + 0.002*\"right\" + 0.002*\"really\" + 0.002*\"pretty\" + 0.002*\"head\" + 0.002*\"want\" + 0.002*\"last\" + 0.002*\"still\" + 0.002*\"enough\" + 0.002*\"as\" + 0.002*\"better\" + 0.002*\"girl\" + 0.002*\"another\" + 0.002*\"nice\" + 0.002*\"tell\" + 0.002*\"shirt\" + 0.002*\"vagina\" + 0.002*\"real\" + 0.002*\"go\" + 0.002*\"said\" + 0.002*\"back\" + 0.002*\"way\" + 0.002*\"god\" + 0.002*\"penis\" + 0.002*\"sign\" + 0.002*\"least\" + 0.002*\"bet\" + 0.002*\"many\" + 0.002*\"man\" + 0.002*\"paper\" + 0.002*\"eye\" + 0.002*\"always\" + 0.002*\"mom\" + 0.002*\"damn\" + 0.002*\"next\" + 0.002*\"ugly\" + 0.002*\"subreddit\" + 0.002*\"try\" + 0.001*\"mr\" + 0.001*\"call\" + 0.001*\"comment\" + 0.001*\"hate\" + 0.001*\"rapist\" + 0.001*\"dude\" + 0.001*\"big\" + 0.001*\"let\"'),\n",
       " (1,\n",
       "  u'0.094*\"you\" + 0.044*\"like\" + 0.042*\"your\" + 0.039*\"look\" + 0.032*\"look_like\" + 0.013*\"it\" + 0.011*\"he\" + 0.008*\"guy\" + 0.007*\"dick\" + 0.007*\"face\" + 0.005*\"one\" + 0.005*\"would\" + 0.005*\"get\" + 0.004*\"kid\" + 0.004*\"girl\" + 0.004*\"got\" + 0.004*\"hair\" + 0.004*\"year\" + 0.004*\"school\" + 0.003*\"make\" + 0.003*\"old\" + 0.003*\"go\" + 0.003*\"already\" + 0.003*\"say\" + 0.003*\"head\" + 0.003*\"friend\" + 0.003*\"child\" + 0.003*\"back\" + 0.003*\"shitty\" + 0.003*\"know\" + 0.003*\"someone\" + 0.002*\"want\" + 0.002*\"bet\" + 0.002*\"going\" + 0.002*\"fat\" + 0.002*\"people\" + 0.002*\"parent\" + 0.002*\"tell\" + 0.002*\"eye\" + 0.002*\"mom\" + 0.002*\"could\" + 0.002*\"think\" + 0.002*\"even\" + 0.002*\"forehead\" + 0.002*\"cum\" + 0.002*\"kind\" + 0.002*\"man\" + 0.002*\"year_old\" + 0.002*\"love\" + 0.002*\"every\" + 0.002*\"left\" + 0.002*\"gay\" + 0.002*\"probably\" + 0.002*\"much\" + 0.002*\"dad\" + 0.002*\"away\" + 0.002*\"type\" + 0.002*\"still\" + 0.002*\"boy\" + 0.002*\"life\" + 0.002*\"really\" + 0.002*\"need\" + 0.002*\"way\" + 0.002*\"pubes\" + 0.002*\"day\" + 0.002*\"put\" + 0.002*\"as\" + 0.002*\"high\" + 0.001*\"trying\" + 0.001*\"white\" + 0.001*\"nigga\" + 0.001*\"brother\" + 0.001*\"fucked\" + 0.001*\"eyebrow\" + 0.001*\"right\" + 0.001*\"thing\" + 0.001*\"asshole\" + 0.001*\"see\" + 0.001*\"woman\" + 0.001*\"give\" + 0.001*\"dude\" + 0.001*\"nose\" + 0.001*\"feel\" + 0.001*\"little\" + 0.001*\"half\" + 0.001*\"beard\" + 0.001*\"body\" + 0.001*\"bad\" + 0.001*\"hand\" + 0.001*\"sister\" + 0.001*\"smell\" + 0.001*\"tit\" + 0.001*\"hit\" + 0.001*\"version\" + 0.001*\"lot\" + 0.001*\"try\" + 0.001*\"top\" + 0.001*\"good\" + 0.001*\"person\" + 0.001*\"mouth\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print Word Topic Distribution for 100 words\n",
    "lda.print_topics(2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "year_old\n",
      "your\n",
      "fuck\n",
      "it\n",
      "love_child\n",
      "first_time\n",
      "he\n",
      "one\n",
      "get\n"
     ]
    }
   ],
   "source": [
    "#Get top 10 terms of topic 0\n",
    "for word_id, prob in lda.get_topic_terms(1,10):\n",
    "    print(dictionary[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.20508158277607622), (1, 0.79491841722392387)]\n",
      "[(0, 0.74158652821235715), (1, 0.25841347178764279)]\n",
      "[(0, 0.36614666211540542), (1, 0.63385333788459464)]\n",
      "[(0, 0.84074959308012098), (1, 0.15925040691987899)]\n",
      "-----------\n",
      "[(0, 0.038192815638495213), (1, 0.91220394550462935), (2, 0.049603238856875523)]\n",
      "[(0, 0.062898839156764252), (1, 0.5585412666898466), (2, 0.37855989415338914)]\n",
      "[(0, 0.068804042712018273), (1, 0.86273325297363546), (2, 0.068462704314346284)]\n",
      "[(0, 0.4616179598475878), (1, 0.0994653893169333), (2, 0.43891665083547893)]\n",
      "-----------\n",
      "[(0, 0.52466008448431378), (1, 0.18010846513473605), (2, 0.14258875440776353), (3, 0.15264269597318664)]\n",
      "[(0, 0.29361461002121242), (1, 0.61508593679948753), (2, 0.047752365065052732), (3, 0.043547088114247311)]\n",
      "[(0, 0.61297664731687818), (1, 0.28576054819107949), (2, 0.050745146727928653), (3, 0.050517657764113669)]\n",
      "[(0, 0.065707889722230017), (1, 0.67535695439257237), (2, 0.067469905052357781), (3, 0.19146525083283991)]\n",
      "-----------\n",
      "[(0, 0.50095804879125605), (1, 0.43791666088465231), (2, 0.020504803803575538), (3, 0.020073712588994267), (4, 0.020546773931521857)]\n",
      "[(0, 0.20403675577616931), (1, 0.42776182778074934), (2, 0.30059373351181912), (3, 0.033887843525620041), (4, 0.033719839405642209)]\n",
      "[(0, 0.59696143020132564), (1, 0.040325340761371668), (2, 0.2822464967152058), (3, 0.040156705427707197), (4, 0.040310026894389915)]\n",
      "[(0, 0.051179663718692571), (1, 0.05181777849459384), (2, 0.050934779656220341), (3, 0.79513928165471981), (4, 0.050928496475773431)]\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "texts = [\"you look like an asshole sitting there in the middle of the room\",\n",
    "        \"stop posting stupid shit like that\",\n",
    "         \"you look like shit\",\n",
    "         \"thank for you help\"]\n",
    "\n",
    "tcorpus = [dictionary.doc2bow(doc.split()) for doc in texts]\n",
    "for docBow in tcorpus:\n",
    "    print(lda_2[docBow]) \n",
    "print(\"-----------\")\n",
    "for docBow in tcorpus:\n",
    "    print(lda_3[docBow]) \n",
    "print(\"-----------\")\n",
    "for docBow in tcorpus:\n",
    "    print(lda_4[docBow]) \n",
    "print(\"-----------\")\n",
    "for docBow in tcorpus:\n",
    "    print(lda_5[docBow]) \n",
    "print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
