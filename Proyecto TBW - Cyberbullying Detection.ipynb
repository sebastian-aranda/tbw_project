{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proyecto Tecnologías de Búsqueda en la Web</h1>\n",
    "<h3>Integrantes</h3>\n",
    "<ul><li>Sebastián Aranda</li><li>Felipe Santander</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Librerías</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary, bleicorpus\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models import Phrases\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.classify import PositiveNaiveBayesClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from random import randint\n",
    "from __future__ import division\n",
    "\n",
    "#import pyLDAvis\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creación del Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Rutas a Datasets</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roastme_dataset_path = \"Dataset/Bullying\"\n",
    "positive_dataset_path = \"Dataset/NoBullying\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Construcción del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset completo:\n",
      "97718\n"
     ]
    }
   ],
   "source": [
    "#Strange symbols\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\\/*^|\"\n",
    "digits = \"0123456789\"\n",
    "\n",
    "#Create bad words list\n",
    "bad_words = []\n",
    "with open('badwords.txt','r') as bad_words_file:\n",
    "    for word in bad_words_file:\n",
    "        word = word.split('-')[0].strip()\n",
    "        word = word.replace('\\n','').decode('unicode_escape').encode('ascii','ignore')\n",
    "        if word != '':\n",
    "            bad_words.append(word)\n",
    "\n",
    "#Create the corpus\n",
    "corpus = list()\n",
    "for name in os.listdir(positive_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(positive_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    #Extract the comment\n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    #Delete links\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]        \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]\n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "positive_dataset_size = len(corpus)\n",
    "\n",
    "for name in os.listdir(roastme_dataset_path):\n",
    "    if name.endswith('.json'):\n",
    "        with open(roastme_dataset_path+'/'+name) as f:\n",
    "            op_json = json.loads(f.read())\n",
    "            try:\n",
    "                for child in op_json[1]['data']['children']:    \n",
    "                    \n",
    "                    #Constraint for dataset size equality\n",
    "                    if (len(corpus) >= 2*positive_dataset_size):\n",
    "                        break\n",
    "                    \n",
    "                    comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                    comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                    for sym in puncts:\n",
    "                        comment_text = comment_text.replace(sym,\" \")\n",
    "                    for num in digits:\n",
    "                        comment_text = comment_text.replace(num,\" \")\n",
    "                    \n",
    "                    tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                    corpus.append(tokens_comment)\n",
    "                        \n",
    "                    try:\n",
    "                        for child in child['data']['replies']['data']['children']:\n",
    "                            \n",
    "                            if (len(corpus) >= 2*positive_dataset_size):\n",
    "                                break\n",
    "                            \n",
    "                            comment_text = child['data']['body'].encode('ascii', 'ignore').replace('\\n', ' ')\n",
    "                            comment_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', comment_text)\n",
    "                    \n",
    "                            for sym in puncts:\n",
    "                                comment_text = comment_text.replace(sym,\" \")\n",
    "                            for num in digits:\n",
    "                                comment_text = comment_text.replace(num,\" \")\n",
    "                        \n",
    "                            tokens_comment = [word for word in comment_text.lower().split()]    \n",
    "                            corpus.append(tokens_comment)\n",
    "                                \n",
    "                    except Exception as e:\n",
    "                        #print(e)\n",
    "                        pass\n",
    "                    \n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "                pass\n",
    "\n",
    "print(\"Tamaño del dataset completo:\")\n",
    "print(len(corpus))\n",
    "#TODO: guardar corpus de texto con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48859\n"
     ]
    }
   ],
   "source": [
    "print(positive_dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Procesamiento del Corpus</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove lone letters\n",
    "corpus = [[word for word in doc if len(word)>1] for doc in corpus]\n",
    "\n",
    "#Remove stopwords\n",
    "corpus = [[word for word in doc if word not in stopwords.words('english')] for doc in corpus]\n",
    "\n",
    "# Lemmatize all words in documents.\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = [[lemmatizer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "\n",
    "#For Sentiment Analysis\n",
    "comments_pro = [\" \".join(doc) for doc in corpus]\n",
    "    \n",
    "#Get Trigrams\n",
    "#bigram = Phrases(corpus)\n",
    "#trigram = Phrases(bigram[corpus])\n",
    "#for idx in range(len(corpus)):\n",
    "#    for token in trigram[corpus[idx]]:\n",
    "#            if '_' in token:\n",
    "#                corpus[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Transformación del Corpus a espacio de vectores</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create dictionary\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "#dictionary.filter_extremes(no_below=len(dictionary)*0.001, no_above=0.75)\n",
    "dictionary.filter_extremes()\n",
    "\n",
    "#Convert documents to vectors\n",
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "#Save corpus and dictionary\n",
    "corpora.BleiCorpus.serialize('tmp/cyberbullying_corpus.lda-c', corpus)\n",
    "dictionary.save('tmp/cyberbullying_dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Creating corpus...\n",
      "Corpus Size: 97718\n"
     ]
    }
   ],
   "source": [
    "#Load the dictionary and corpus\n",
    "if (os.path.exists('tmp/cyberbullying_dictionary.dict') and os.path.exists('tmp/cyberbullying_corpus.lda-c')):\n",
    "    print('Creating dictionary...')\n",
    "    dictionary = corpora.Dictionary.load('tmp/cyberbullying_dictionary.dict')\n",
    "    print('Creating corpus...')\n",
    "    print('Corpus Size: '+str(len(corpus)))\n",
    "    corpus = corpora.BleiCorpus('tmp/cyberbullying_corpus.lda-c')\n",
    "else:\n",
    "    print(\"Create the dictionary and corpus first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Características para Clasificación</h1>\n",
    "<ul>\n",
    "<li>Porcentaje de pertenencia al tópico generado de LDA en Roastme Dataset</li>\n",
    "<li>Densidad de Badwords</li>\n",
    "<li>Polaridad del post utilizando Sentiment Analysis</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>LDA Features</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Modelo</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 616 ms, total: 2min 20s\n",
      "Wall time: 2min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.015*\"like\" + 0.012*\"look\" + 0.011*\"roast\" + 0.009*\"fuck\" + 0.008*\"post\" + 0.006*\"one\" + 0.006*\"dick\" + 0.006*\"guy\" + 0.006*\"picture\" + 0.006*\"know\" + 0.006*\"please\" + 0.005*\"would\" + 0.005*\"sure\" + 0.005*\"get\" + 0.004*\"forehead\" + 0.004*\"use\" + 0.004*\"let\" + 0.004*\"never\" + 0.004*\"nice\" + 0.004*\"see\" + 0.004*\"fat\" + 0.004*\"sex\" + 0.004*\"subreddit\" + 0.004*\"u\" + 0.003*\"got\" + 0.003*\"gay\" + 0.003*\"love\" + 0.003*\"comment\" + 0.003*\"as\" + 0.003*\"report\" + 0.003*\"want\" + 0.003*\"another\" + 0.003*\"deleted\" + 0.003*\"say\" + 0.003*\"bitch\" + 0.003*\"rule\" + 0.003*\"user\" + 0.003*\"probably\" + 0.003*\"moderator\" + 0.003*\"dude\"'),\n",
       " (1,\n",
       "  u'0.045*\"like\" + 0.043*\"look\" + 0.013*\"face\" + 0.009*\"one\" + 0.009*\"get\" + 0.008*\"hair\" + 0.007*\"guy\" + 0.007*\"make\" + 0.006*\"would\" + 0.006*\"girl\" + 0.006*\"kid\" + 0.006*\"think\" + 0.005*\"even\" + 0.005*\"time\" + 0.005*\"school\" + 0.005*\"tell\" + 0.005*\"go\" + 0.005*\"year\" + 0.005*\"got\" + 0.004*\"thing\" + 0.004*\"head\" + 0.004*\"back\" + 0.004*\"people\" + 0.004*\"know\" + 0.004*\"already\" + 0.004*\"say\" + 0.004*\"going\" + 0.004*\"life\" + 0.004*\"right\" + 0.004*\"bet\" + 0.004*\"good\" + 0.004*\"shit\" + 0.004*\"still\" + 0.004*\"see\" + 0.004*\"could\" + 0.004*\"take\" + 0.004*\"need\" + 0.003*\"eye\" + 0.003*\"mom\" + 0.003*\"friend\"')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize a model\n",
    "#print('Creating Tfidf model...')\n",
    "#tfidf = gensim.models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "#lda_model_tfidf = ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary)\n",
    "\n",
    "#LDA-2\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=2, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(2,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-3\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(3,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-5\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(5,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA-10\n",
    "%time lda_model = ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary)\n",
    "lda_model.save('tmp/cyberbullying_ldaModel.lda')\n",
    "lda_model.print_topics(10,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creación de características</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_mostAgressiveTopic = 0; \n",
    "\n",
    "lda_features = []\n",
    "for docbow in corpus:\n",
    "    lda_features.append(lda_model[docbow][id_mostAgressiveTopic][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Bad Words Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_words_ids = [word_id for word_id, word in dictionary.iteritems() if word in bad_words]\n",
    "bad_words_features = []\n",
    "\n",
    "for docbow in corpus:\n",
    "        if (len(docbow) > 0):\n",
    "            bad_words_features.append(sum([freq for word_id,freq in docbow if word_id in bad_words_ids])/len(docbow))\n",
    "        else:\n",
    "            bad_words_features.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentiment Analysis Features</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_dataset_size = int(len(corpus)/2)\n",
    "positive_comments = [comments_pro[i] for i in range(positive_dataset_size)]\n",
    "negative_comments = [comments_pro[i] for i in range(positive_dataset_size,len(corpus))]\n",
    "\n",
    "def features(list_of_comments):\n",
    "    words = list_of_comments.lower().split()\n",
    "    return dict(('contains(%s)' % w, True) for w in words)\n",
    "\n",
    "positive_featuresets = list(map(features, negative_comments)) + list(map(features, bad_words))\n",
    "#positive_featuresets = list(map(features, bad_words))\n",
    "unlabeled_featuresets = list(map(features, positive_comments))\n",
    "classifier = PositiveNaiveBayesClassifier.train(positive_featuresets, unlabeled_featuresets)\n",
    "\n",
    "sentiment_features = []\n",
    "for doc in comments_pro:\n",
    "    sentiment_features.append(classifier.classify(features(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Validación Cruzada</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Etiquetas</h3>\n",
    "<p>Asignar etiqueta segun corpus</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97718\n"
     ]
    }
   ],
   "source": [
    "positive_dataset_size = int(len(corpus)/2)\n",
    "y1 = [0 for label in range(positive_dataset_size)]\n",
    "y2 = [1 for label in range(positive_dataset_size,len(corpus))]\n",
    "y = y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastian/anaconda3/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747423358625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = zip(lda_features,bad_words_features, sentiment_features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .7)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "myClassifier = KNeighborsClassifier()\n",
    "myClassifier.fit(X_train, y_train)\n",
    "predictions = myClassifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
